% train a rtrbm on time sequence of ode
%
% remark [Xiuyuan, 25/06/2015]: 
%       in the nips08 paper, the video generated by
%       RTRBM and TRBM (maybe poorly trained) all have "Brownian-motion like"
%       hesitating motions. And in the 2014 strRTRBM paper, the motion is
%       more directed. Can the "hesitating motion" be due to the poorly
%       regularized W, which affects the learning of W and U both? Or it is
%       actually due to that {r_t}_t are conditioned so that the
%       negative/adverserial samples are not sufficiently explored. Or,
%       maybe not enough hidden nodes (then what about over-fitting)?

addpath ../utils/

%% data

%%% translating square wave
% size = [d_v, T, num_samples]

data = video_bouncing_1d();
[d_v, T, num_samples] = size( data);

% hh = 2*pi/T; tt  = hh/2:hh:2*pi;
% for i = 1: num_samples
%     tau = rand(1)*2*pi;
%     data(:, :, i) = [ sin(tt + tau); cos(tt + tau)];
% end



%% para of rtrbm
train.learning_rate             =  0.01;
train.momentum                  =  0.9;
train.weightcost                =  0.0002;

train.CDk                       =  10;
train.maxepoch                  =  1000;
train.batchsize                 =  128;


rtrbm.type                      = 'binary';
rtrbm.d_h                       =  16;  %number of hidden node, 1-layer RBM
rtrbm.d_v                       =  d_v; %number of visible nodes
rtrbm.T                         =  T; % fix time sequence length

% initial of network parameter
rtrbm.W                         = 0.1*randn( rtrbm.d_h, rtrbm.d_v);              
rtrbm.U                         = 0.1*randn( rtrbm.d_h, rtrbm.d_h);              
rtrbm.b                         = zeros( rtrbm.d_h, 1);
rtrbm.c                         = zeros( rtrbm.d_v, 1);
rtrbm.b0                        = zeros( rtrbm.d_h, 1);

rtrbm.vW                         = zeros( rtrbm.d_h, rtrbm.d_v);
rtrbm.vU                         = zeros( rtrbm.d_h, rtrbm.d_h);
rtrbm.vb                         = zeros( rtrbm.d_h, 1);
rtrbm.vc                         = zeros( rtrbm.d_v, 1);
rtrbm.vb0                        = zeros( rtrbm.d_h, 1);


%% train rtrbm by bp, train by batch
% theta = { W, U, b, c, b0 }

epoch = 1;

%%
num_batches = floor( num_samples/train.batchsize ); %assume integer

for  epoch = epoch: train.maxepoch
    
    fprintf( 'epo %d/%d: ', epoch, train.maxepoch);
    
    err = 0;
    str = '';
    
    for batch = 1: num_batches
        
        for tmp=1:length( str)
            fprintf('\b');
        end
        str = sprintf('processing batch %d/%d', batch, num_batches);
        fprintf( str);
        
        rtrbm.dW                         = zeros( rtrbm.d_h, rtrbm.d_v);
        rtrbm.dU                         = zeros( rtrbm.d_h, rtrbm.d_h);
        rtrbm.db                         = zeros( rtrbm.d_h, 1);
        rtrbm.dc                         = zeros( rtrbm.d_v, 1);
        rtrbm.db0                        = zeros( rtrbm.d_h, 1);
        
        
        for i = 1: train.batchsize 
            
           
            %% let vt be the data
            % size of vt = [d_v, T]
            vt = data(:, :, (batch-1)*train.batchsize+i);
            
            %% compute the auxilary numbers
            % given {vt}_t, compute {rt}_t
            rt          = zeros( rtrbm.d_h, rtrbm.T);
            rt(:, 1)    = sigm( rtrbm.W * vt(:,1)  + rtrbm.b0  );
            for t = 2: rtrbm.T
                rt(:, t) = sigm( rtrbm.W * vt(:,t)  + rtrbm.b + rtrbm.U *rt(:, t-1));
            end
            
            % compute {\bar ht}_t, barht = E_{{ht},{v't}|{rt}} ht
            %         {\bar vt}_t, barvt = E_{{ht},{v't}|{rt}} vt
            % by gibbs sampling, the sequence are saved in ht_k, vt_k for k steps.
            % size( ht_k) = [d_h, T, k], size( vt_k) = [d_v, T, k],
            ht_k = zeros( rtrbm.d_h, rtrbm.T, train.CDk);
            probht_k = zeros( rtrbm.d_h, rtrbm.T, train.CDk);
            vt_k = zeros( rtrbm.d_v, rtrbm.T, train.CDk);
            
            % gibbs sampling of CDk steps
            vt_k(:, :, 1) = vt;
            probht_k(:, 1, 1) = sigm( rtrbm.W * vt_k(:, 1, 1)  + rtrbm.b0  );
            probht_k(:, 2:rtrbm.T, 1) = reshape(...
                sigm( rtrbm.W * reshape( vt_k(:, 2:rtrbm.T, 1), [rtrbm.d_v, rtrbm.T-1]) ...
                + repmat( rtrbm.b, [1, rtrbm.T-1 ]) ...
                + rtrbm.U * rt(:,1:rtrbm.T-1) ), ...
                [rtrbm.d_h, rtrbm.T-1] );
            ht_k(:, :, 1) = rand( rtrbm.d_h, rtrbm.T) <  probht_k(:, :, 1);
            
            for kk = 2: train.CDk
                vt_k(:, :, kk) = sigmrnd( (rtrbm.W).'* ht_k(:, :, kk-1) ...
                                         + repmat( rtrbm.c, [1, rtrbm.T ]));
                
                probht_k(:, 1, kk) = sigm( rtrbm.W * vt_k(:, 1, kk)  + rtrbm.b0  );
                probht_k(:, 2:rtrbm.T, kk) = reshape(...
                    sigm( rtrbm.W * reshape( vt_k(:, 2:rtrbm.T, kk), [rtrbm.d_v, rtrbm.T-1]) ...
                    + repmat( rtrbm.b, [1, rtrbm.T-1 ]) ...
                    + rtrbm.U * rt(:,1:rtrbm.T-1) ), ...
                    [rtrbm.d_h, rtrbm.T-1] );
                
                ht_k(:, :, kk) = rand( rtrbm.d_h, rtrbm.T) <  probht_k(:, :, kk);
            end
            
            barht = mean( probht_k, 3);
            barvt = mean( vt_k, 3);
            
            
            % compute the auxilary D_t, for t = T, T-1, ..., 2 via back propagation
            %   D_{T+1} = 0
            %   D_t     = U^T*{ D_{t+1}.*r_t.*(1-r_t)
            %                  + ( \E_{h|v,r}ht - \E_{h,v'|r}ht ) }
            %  where \E_{h|v,r}ht = rt
            Dt         = zeros( rtrbm.d_h, rtrbm.T+1);
            for t = T:-1:2
                Dt(:, t) = (rtrbm.U).'*( Dt(:,t+1).*rt(:,t).*(1- rt(:,t)) ...
                                        + (rt(:,t) - barht(:, t)) );
            end
            
            %% compute the updates
            
            % the update of b0
            %   upd_b0      = upd_b0_1 + upd_b0_2
            %   upd_b0_1    =  \E_{h|v,r} h1 - \E_{h,v'|r} h1
            %   upd_b0_2    = D2.* r1.*(1-r1)
            rtrbm.db0   = rtrbm.db0 + ...
                (rt(:,1) - barht(:,1)) + Dt(:,2).*rt(:,1).*(1-rt(:,1));
            
            
            % the update of b
            %   upd_b       = upd_b_1 + upd_b_2
            %   upd_b_1     = ( \E_{h|v,r} - \E_{h,v'|r}) \sum_{t=2}^T ht
            %   upd_b_2     = \sum_{t=2}^{T-1} D_{t+1}.*rt.*(1-rt)
            tmp = sum( Dt(:, 3:rtrbm.T).*( rt(:,2:rtrbm.T-1).*(1 - rt(:,2:rtrbm.T-1)) ), 2);
            rtrbm.db    =  rtrbm.db+ ...
                sum(rt(:, 2:rtrbm.T), 2) - sum(barht(:, 2:rtrbm.T), 2) + tmp;
            
            % the update of c
            %   upd_c       = upd_c_1 + upd_c_2
            %   upd_c_1     = ( \E_{h|v,r} - \E_{h,v'|r}) \sum_{t=1}^T vt
            %   upd_c_2     = 0
            rtrbm.dc    = rtrbm.dc  + ...
                sum( vt - barvt ,2);
            
            % the update of W
            %   upd_W       = upd_W_1 + upd_W_2
            %   upd_W_1     = ( \E_{h|v,r} - \E_{h,v'|r}) \sum_{t=1}^T ht*vt^T
            %   upd_W_2     = \sum_{t=1}^{T-1} (D_{t+1}.*rt.*(1-rt))*vt^T
            %                  (vt is clamped to data distribution, as it comes from rt which is "conditioned on")
            
            upd_W_2 = sum( repmat( reshape( Dt(:, 2:rtrbm.T).*( rt(:,1:rtrbm.T-1).*(1 - rt(:,1:rtrbm.T-1) ) ), ...
                                            [rtrbm.d_h, 1, rtrbm.T-1] ), ...
                                  [1 rtrbm.d_v 1]) .* ...
                           repmat( reshape( vt(:, 1: rtrbm.T-1), [1 rtrbm.d_v rtrbm.T-1]), ...
                                  [rtrbm.d_h, 1 1]), 3);
            upd_W_1 = sum( repmat( reshape(   rt, [rtrbm.d_h, 1, rtrbm.T]), [1, rtrbm.d_v, 1] ) .* ...
                           repmat( reshape(   vt, [1, rtrbm.d_v, rtrbm.T]), [rtrbm.d_h, 1, 1] ), 3)...
                    - sum( repmat( reshape( ht_k, [rtrbm.d_h, 1, rtrbm.T*train.CDk]), [1, rtrbm.d_v, 1]).*...
                           repmat( reshape( vt_k, [1, rtrbm.d_v, rtrbm.T*train.CDk]), [rtrbm.d_h, 1, 1]), 3)/train.CDk;
            rtrbm.dW =  rtrbm.dW + ...
                upd_W_1 + upd_W_2;
            
            
            % the update of U
            %   upd_U_1 = 0,
            %   upd_U_2 = \sum_{t=2}^T (D_{t+1}.*rt.*(1-rt) + (rt - \barht) )*r_{t-1}^T
            rtrbm.dU =  rtrbm.dU  + sum( repmat( reshape( ...
                Dt(:, 3:rtrbm.T+1).* ( rt(:,2:rtrbm.T).*(1 - rt(:,2:rtrbm.T)) ) ...
                + rt(:, 2:rtrbm.T) - barht(:, 2:rtrbm.T), ...
                [rtrbm.d_h, 1, rtrbm.T-1] ), [1, rtrbm.d_h, 1] )...
                .*repmat( reshape( rt(:, 1:rtrbm.T-1), [1, rtrbm.d_h, rtrbm.T-1] ), [rtrbm.d_h, 1, 1] ), 3);
            
            
            %% compute l2 err
            tmp = vt - vt_k(:, :, end);
            err = err +  sum(tmp(:).^ 2)/rtrbm.T ;
            
        end
        
        %% update the weight
        
        rtrbm.vW  = train.momentum *  rtrbm.vW  + ...
            train.learning_rate * ( rtrbm.dW/train.batchsize  - train.weightcost * rtrbm.W );
        
        rtrbm.vU  = train.momentum *  rtrbm.vU  + ...
            train.learning_rate * ( rtrbm.dU/train.batchsize  - train.weightcost * rtrbm.U );
        
        rtrbm.vb  = train.momentum *  rtrbm.vb  + ...
            train.learning_rate * rtrbm.db/train.batchsize ;
        
        rtrbm.vc  = train.momentum *  rtrbm.vc  + ...
            train.learning_rate * rtrbm.dc/train.batchsize ;
        
        rtrbm.vb0  = train.momentum *  rtrbm.vb0  + ...
            train.learning_rate * rtrbm.db0/train.batchsize ;
        
        
        rtrbm.W = rtrbm.W + rtrbm.vW;
        rtrbm.U = rtrbm.U + rtrbm.vU;
        rtrbm.b = rtrbm.b + rtrbm.vb;
        rtrbm.c = rtrbm.c + rtrbm.vc;
        rtrbm.b0 = rtrbm.b0 + rtrbm.vb0;
       
    end
    
    fprintf( '. l2 error %4.2f\n', err/num_samples);
end

%% measure the prediction error
T0 = 8;
for ii = 2: 2 : 250
    
    [vt1, rt1] = predict_rtrbm_1d( data(:,1:T0, ii),  rtrbm);
    
    figure(1), clf
    subplot( 1,2,1)
    imagesc( data( :, :, ii)), title('true')
    colormap(1-gray);
    subplot( 1,2,2)
    imagesc( vt1), title('generated');
    colormap(1-gray);
    pause();
end
%%

T1 = 8;
figure(2), 
for t = 1: T1
    clf,
    plot( vt1(:, t), '.-');
    hold on;
    plot( data(:, t, ii), 'x-r');
    legend('predict', 'true');
    title(sprintf('t=%d',t));
    pause();
end

%% visualize generated sequence conditioned on data-clamped {rt}_t,
%    equivalent to a RBM at each time t, independently across t
figure(1), clf
for kk = 1: train.CDk
    for t = 1: T
        plot( vt_k(:, t, kk), '.-');
        title( sprintf( 'gibbs k=%d', kk));
        pause(0.1);
    end
    pause();
end


%%%
% for 1d binary translation wave, with d_v = 16, there are very few ways to
% vary the signal (< 100 samples in total) and thus easitly overfit the
% system. It could make sense to study 
%   1)  why with signals of both left and right translation, the machine
%       fails to predict translation? why the "direction" info cannot be
%       installed in rt?
%   2)  why with signals of varying plate-size and propagation speed, the
%       machine fails to learn the size and speed of the propagation wave,
%       and got confused in predicting?
% the fix may lie in: 
%   - more hidden nodes
%   - (nn architecture change) more hidden layers, go conv to increase
%   stability
%   - go bouncing, real-valued (G-RBM) or go to 2d signals so that there are more
%   samples

